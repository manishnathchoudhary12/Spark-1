{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+------+----+-----+------+-----------+----+----+-----------+\n",
      "|BAD|LOAN|MORTDUE| VALUE| REASON|   JOB| YOJ|DEROG|DELINQ|      CLAGE|NINQ|CLNO|    DEBTINC|\n",
      "+---+----+-------+------+-------+------+----+-----+------+-----------+----+----+-----------+\n",
      "|  1|1100|  25860| 39025|HomeImp| Other|10.5|    0|     0|94.36666667|   1|   9|       null|\n",
      "|  1|1300|  70053| 68400|HomeImp| Other|   7|    0|     2|121.8333333|   0|  14|       null|\n",
      "|  1|1500|  13500| 16700|HomeImp| Other|   4|    0|     0|149.4666667|   1|  10|       null|\n",
      "|  1|1500|   null|  null|   null|  null|null| null|  null|       null|null|null|       null|\n",
      "|  0|1700|  97800|112000|HomeImp|Office|   3|    0|     0|93.33333333|   0|  14|       null|\n",
      "|  1|1700|  30548| 40320|HomeImp| Other|   9|    0|     0|101.4660019|   1|   8|37.11361356|\n",
      "|  1|1800|  48649| 57037|HomeImp| Other|   5|    3|     2|       77.1|   1|  17|       null|\n",
      "|  1|1800|  28502| 43034|HomeImp| Other|  11|    0|     0|88.76602988|   0|   8|36.88489409|\n",
      "|  1|2000|  32700| 46740|HomeImp| Other|   3|    0|     2|216.9333333|   1|  12|       null|\n",
      "|  1|2000|   null| 62250|HomeImp| Sales|  16|    0|     0|      115.8|   0|  13|       null|\n",
      "+---+----+-------+------+-------+------+----+-----+------+-----------+----+----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create Spark Session\n",
    "spark=SparkSession.builder.appName(\"simple\").getOrCreate()\n",
    "\n",
    "#Read in text file from HDFS\n",
    "HMEQ = spark.read.csv('hdfs://localhost:54310/user/andrew/data/HMEQ.csv', header=True)\n",
    "\n",
    "#Register for SQL Use\n",
    "HMEQ.createOrReplaceTempView(\"HMEQ_SQL\")\n",
    "\n",
    "HMEQ.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmeq1 = spark.sql('''SELECT * \n",
    "                     FROM HMEQ_SQL\n",
    "                     WHERE BAD =0''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+-------+----+-----+------+-----------+----+----+-----------+\n",
      "|BAD|LOAN|MORTDUE| VALUE| REASON|    JOB| YOJ|DEROG|DELINQ|      CLAGE|NINQ|CLNO|    DEBTINC|\n",
      "+---+----+-------+------+-------+-------+----+-----+------+-----------+----+----+-----------+\n",
      "|  0|1700|  97800|112000|HomeImp| Office|   3|    0|     0|93.33333333|   0|  14|       null|\n",
      "|  0|2000|  64536| 87400|   null|    Mgr| 2.5|    0|     0|147.1333333|   0|  24|       null|\n",
      "|  0|2300| 102370|120953|HomeImp| Office|   2|    0|     0|90.99253347|   0|  13|31.58850318|\n",
      "|  0|2400|  98449|117195|HomeImp| Office|   4|    0|     0|93.81177485|   0|  13|29.68182705|\n",
      "|  0|2500|   7229| 44516|HomeImp|   Self|null|    0|     0|        208|   0|  12|       null|\n",
      "|  0|2500|  71408| 78600|HomeImp|ProfExe|   8|    0|     0|255.7333333|   0|  12|       null|\n",
      "|  0|2900| 103949|112505|HomeImp| Office|   1|    0|     0|96.10232967|   0|  13|30.05113629|\n",
      "|  0|2900| 104373|120702|HomeImp| Office|   2|    0|     0|101.5402975|   0|  13|29.91585903|\n",
      "|  0|3000| 104570|121729|HomeImp| Office|   2|    0|     0|85.88437189|   0|  14|32.05978327|\n",
      "|  0|3000|  58000| 71500|HomeImp|    Mgr|  10| null|     2|211.9333333|   0|  25|       null|\n",
      "+---+----+-------+------+-------+-------+----+-----+------+-----------+----+----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hmeq1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register as Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hive_context=HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-0aa6cc22-d440-4f8e-86ef-491a881b995c/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 4, in <module>\n",
       "  File \"/home/andrew/spark/python/pyspark/sql/context.py\", line 384, in sql\n",
       "    return self.sparkSession.sql(sqlQuery)\n",
       "  File \"/home/andrew/spark/python/pyspark/sql/session.py\", line 541, in sql\n",
       "    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
       "  File \"/home/andrew/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n",
       "    answer, self.gateway_client, self.target_id, self.name)\n",
       "  File \"/home/andrew/spark/python/pyspark/sql/utils.py\", line 69, in deco\n",
       "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
       "AnalysisException: u'Hive support is required to use CREATE Hive TABLE AS SELECT;;\\nCreateTable CatalogTable(\\n\\tTable: `hmeq2`\\n\\tCreated: Mon Jul 03 14:19:02 EDT 2017\\n\\tLast Access: Wed Dec 31 18:59:59 EST 1969\\n\\tType: MANAGED\\n\\tProvider: hive\\n\\tStorage(InputFormat: org.apache.hadoop.mapred.TextInputFormat, OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat)), ErrorIfExists\\n   +- Project [BAD#0, LOAN#1, MORTDUE#2, VALUE#3, REASON#4, JOB#5, YOJ#6, DEROG#7, DELINQ#8, CLAGE#9, NINQ#10, CLNO#11, DEBTINC#12]\\n      +- Filter (cast(BAD#0 as double) = cast(0 as double))\\n         +- SubqueryAlias hmeq_sql, `hmeq_SQL`\\n            +- Relation[BAD#0,LOAN#1,MORTDUE#2,VALUE#3,REASON#4,JOB#5,YOJ#6,DEROG#7,DELINQ#8,CLAGE#9,NINQ#10,CLNO#11,DEBTINC#12] csv\\n'\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive_context.sql('''CREATE TABLE hmeq2 \n",
    "             AS SELECT * \n",
    "             FROM hmeq_SQL \n",
    "             WHERE BAD = 0''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        | hmeq_sql|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.12\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
