{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+------+----+-----+------+-----------+----+----+-----------+\n",
      "|BAD|LOAN|MORTDUE| VALUE| REASON|   JOB| YOJ|DEROG|DELINQ|      CLAGE|NINQ|CLNO|    DEBTINC|\n",
      "+---+----+-------+------+-------+------+----+-----+------+-----------+----+----+-----------+\n",
      "|  1|1100|  25860| 39025|HomeImp| Other|10.5|    0|     0|94.36666667|   1|   9|       null|\n",
      "|  1|1300|  70053| 68400|HomeImp| Other|   7|    0|     2|121.8333333|   0|  14|       null|\n",
      "|  1|1500|  13500| 16700|HomeImp| Other|   4|    0|     0|149.4666667|   1|  10|       null|\n",
      "|  1|1500|   null|  null|   null|  null|null| null|  null|       null|null|null|       null|\n",
      "|  0|1700|  97800|112000|HomeImp|Office|   3|    0|     0|93.33333333|   0|  14|       null|\n",
      "|  1|1700|  30548| 40320|HomeImp| Other|   9|    0|     0|101.4660019|   1|   8|37.11361356|\n",
      "|  1|1800|  48649| 57037|HomeImp| Other|   5|    3|     2|       77.1|   1|  17|       null|\n",
      "|  1|1800|  28502| 43034|HomeImp| Other|  11|    0|     0|88.76602988|   0|   8|36.88489409|\n",
      "|  1|2000|  32700| 46740|HomeImp| Other|   3|    0|     2|216.9333333|   1|  12|       null|\n",
      "|  1|2000|   null| 62250|HomeImp| Sales|  16|    0|     0|      115.8|   0|  13|       null|\n",
      "+---+----+-------+------+-------+------+----+-----+------+-----------+----+----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "//Create Spark Session\n",
    "val spark = SparkSession.builder().appName(\"simple\").getOrCreate()\n",
    "\n",
    "//Read in data\n",
    "val hmeq = spark.read.option(\"header\",\"true\").csv(\"/hadoop_work/data/HMEQ.csv\")\n",
    "hmeq.show(10)\n",
    "\n",
    "//register table to use in SparkSql\n",
    "hmeq.createOrReplaceTempView(\"hmeq1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hmeq_one = spark.sql(\"SELECT * FROM hmeq1 WHERE BAD = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+-------+----+-----+------+-----------+----+----+-----------+\n",
      "|BAD|LOAN|MORTDUE| VALUE| REASON|    JOB| YOJ|DEROG|DELINQ|      CLAGE|NINQ|CLNO|    DEBTINC|\n",
      "+---+----+-------+------+-------+-------+----+-----+------+-----------+----+----+-----------+\n",
      "|  0|1700|  97800|112000|HomeImp| Office|   3|    0|     0|93.33333333|   0|  14|       null|\n",
      "|  0|2000|  64536| 87400|   null|    Mgr| 2.5|    0|     0|147.1333333|   0|  24|       null|\n",
      "|  0|2300| 102370|120953|HomeImp| Office|   2|    0|     0|90.99253347|   0|  13|31.58850318|\n",
      "|  0|2400|  98449|117195|HomeImp| Office|   4|    0|     0|93.81177485|   0|  13|29.68182705|\n",
      "|  0|2500|   7229| 44516|HomeImp|   Self|null|    0|     0|        208|   0|  12|       null|\n",
      "|  0|2500|  71408| 78600|HomeImp|ProfExe|   8|    0|     0|255.7333333|   0|  12|       null|\n",
      "|  0|2900| 103949|112505|HomeImp| Office|   1|    0|     0|96.10232967|   0|  13|30.05113629|\n",
      "|  0|2900| 104373|120702|HomeImp| Office|   2|    0|     0|101.5402975|   0|  13|29.91585903|\n",
      "|  0|3000| 104570|121729|HomeImp| Office|   2|    0|     0|85.88437189|   0|  14|32.05978327|\n",
      "|  0|3000|  58000| 71500|HomeImp|    Mgr|  10| null|     2|211.9333333|   0|  25|       null|\n",
      "|  0|3000|  47000| 56400|HomeImp| Office|  21|    0|     0|201.8666667|   0|  20|       null|\n",
      "|  0|3100|   null| 70400|   null|   null|null| null|  null|       null|null|null|       null|\n",
      "|  0|3200|  67848| 74566|HomeImp|    Mgr|  10| null|     1|206.4266021|   0|  25|40.11567728|\n",
      "|  0|3200|  74864| 87266|HomeImp|ProfExe|   7|    0|     0|250.6312692|   0|  12|42.90999735|\n",
      "|  0|3600|  83700|111800|HomeImp|ProfExe|   6|    0|     0|192.7333333|   1|  28|       null|\n",
      "|  0|3600|  61327| 76484|HomeImp|    Mgr|   9| null|     2|202.5107752|   0|  25| 41.5163897|\n",
      "|  0|3600| 100693|114743|HomeImp| Office|   6|    0|     0|88.47045213|   0|  14|29.39354338|\n",
      "|  0|3600|  52337| 63989|HomeImp| Office|  20|    0|     0|204.2724988|   0|  20|20.47091551|\n",
      "|  0|3800|  51180| 63459|HomeImp| Office|  20|    0|     0|203.7515336|   0|  20|20.06704205|\n",
      "|  0|3800|   null| 73189|   null|   null|null| null|  null|       null|null|null|22.25394916|\n",
      "+---+----+-------+------+-------+-------+----+-----+------+-----------+----+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hmeq_one.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register as Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Hive support is required to use CREATE Hive TABLE AS SELECT;;\n",
       "CreateTable CatalogTable(\n",
       "\tTable: `hmeq2`\n",
       "\tCreated: Mon Jul 03 09:40:15 EDT 2017\n",
       "\tLast Access: Wed Dec 31 18:59:59 EST 1969\n",
       "\tType: MANAGED\n",
       "\tProvider: hive\n",
       "\tStorage(InputFormat: org.apache.hadoop.mapred.TextInputFormat, OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat)), ErrorIfExists\n",
       "   +- Project [BAD#55, LOAN#56, MORTDUE#57, VALUE#58, REASON#59, JOB#60, YOJ#61, DEROG#62, DELINQ#63, CLAGE#64, NINQ#65, CLNO#66, DEBTINC#67]\n",
       "      +- Filter (cast(BAD#55 as double) = cast(0 as double))\n",
       "         +- SubqueryAlias hmeq1, `hmeq1`\n",
       "            +- Relation[BAD#55,LOAN#56,MORTDUE#57,VALUE#58,REASON#59,JOB#60,YOJ#61,DEROG#62,DELINQ#63,CLAGE#64,NINQ#65,CLNO#66,DEBTINC#67] csv\n",
       "\n",
       "StackTrace: org.apache.spark.sql.AnalysisException: Hive support is required to use CREATE Hive TABLE AS SELECT;;\n",
       "CreateTable CatalogTable(\n",
       "\tTable: `hmeq2`\n",
       "\tCreated: Mon Jul 03 09:40:15 EDT 2017\n",
       "\tLast Access: Wed Dec 31 18:59:59 EST 1969\n",
       "\tType: MANAGED\n",
       "\tProvider: hive\n",
       "\tStorage(InputFormat: org.apache.hadoop.mapred.TextInputFormat, OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat)), ErrorIfExists\n",
       "   +- Project [BAD#55, LOAN#56, MORTDUE#57, VALUE#58, REASON#59, JOB#60, YOJ#61, DEROG#62, DELINQ#63, CLAGE#64, NINQ#65, CLNO#66, DEBTINC#67]\n",
       "      +- Filter (cast(BAD#55 as double) = cast(0 as double))\n",
       "         +- SubqueryAlias hmeq1, `hmeq1`\n",
       "            +- Relation[BAD#55,LOAN#56,MORTDUE#57,VALUE#58,REASON#59,JOB#60,YOJ#61,DEROG#62,DELINQ#63,CLAGE#64,NINQ#65,CLNO#66,DEBTINC#67] csv\n",
       "\n",
       "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$$anonfun$apply$4.apply(rules.scala:297)\n",
       "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$$anonfun$apply$4.apply(rules.scala:294)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:118)\n",
       "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:294)\n",
       "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:292)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$2.apply(CheckAnalysis.scala:425)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$2.apply(CheckAnalysis.scala:425)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:425)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n",
       "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table hmeq2 AS SELECT * FROM hmeq1 WHERE BAD = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |    hmeq1|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
