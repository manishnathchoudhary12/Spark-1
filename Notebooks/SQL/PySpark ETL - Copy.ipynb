{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark ETL\n",
    "This example will show how to use SparkSql to perform ETL on a set of relational tables\n",
    "<br>\n",
    "<br>\n",
    "**Tables**:\n",
    "1. Customer: Information such as State, Gender and Customer ID\n",
    "2. Product: List of SKUs the retail chain s ells\n",
    "3. Transaction: Point of Sale (POS) data for customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, regexp_replace, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#Create Spark Session\n",
    "spark=SparkSession.builder.appName(\"simple\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Files from hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+-----------+--------------------+----------+--------------------+--------------+\n",
      "|CustomerID|State|Gender|  FirstName|            LastName|Birth_Date|             Address|Member_Type_ID|\n",
      "+----------+-----+------+-----------+--------------------+----------+--------------------+--------------+\n",
      "|         1|   KY|     M|     Albert|              Collet| 24Nov1940|Square Edouard Vii 1|          2030|\n",
      "|         2|   ME|     F|   Mercedes|            Mart�nez| 15Jan1955|          Edificio 2|          1010|\n",
      "|         3|   IN|     M|Pier Egidio|              Boeris| 01Jul1970|Via M. Di Monteso...|          1040|\n",
      "|         4|   NY|     M|      James|             Kvarniq| 27Jun1970|      4382 Gralyn Rd|          1020|\n",
      "|         5|   NY|     F|   Sandrina|            Stephano| 09Jul1975|    6468 Cog Hill Ct|          2020|\n",
      "|         6|   OH|     M|       Rent|            Van Lint| 23Dec1945|         Mispadstr 2|          1030|\n",
      "|         7|   ME|     F|     Juli�n|Escorihuela Monse...| 07Aug1975|Co. De Los Clavel...|          1040|\n",
      "|         8|   KY|     M|        Aki|              Ivonen| 04Dec1935|         Valimotie 5|          1020|\n",
      "|         9|   OH|     F|   Cornelia|               Krahl| 27Feb1970|   Kallstadterstr. 9|          2020|\n",
      "|        10|   NY|     F|      Karen|           Ballinger| 18Oct1980|425 Bryant Estate...|          1040|\n",
      "+----------+-----+------+-----------+--------------------+----------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read in Customer file from HDFS\n",
    "Customer = spark.read.csv('hdfs://localhost:54310/user/andrew/data/relational_example/Customer.csv', header=True)\n",
    "\n",
    "#Register for SQL Use\n",
    "Customer.createOrReplaceTempView(\"Customer\")\n",
    "\n",
    "#See Head\n",
    "Customer.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------+\n",
      "|itemcode|                item|   category|Amount|\n",
      "+--------+--------------------+-----------+------+\n",
      "|     111|             SEP IRA| Retirement| 550.0|\n",
      "|     112|              Keough| Retirement| 325.0|\n",
      "|     113|          Simple IRA| Retirement| 125.0|\n",
      "|     114|         US Equities|Mutual Fund|   2.8|\n",
      "|     115|International Equ...|Mutual Fund| 325.0|\n",
      "|     116|      Lifestyle Fund|Mutual Fund| 245.0|\n",
      "|     117|        Money Market|Mutual Fund| 268.0|\n",
      "|     118|Mutual Fund - US ...|Mutual Fund|  75.0|\n",
      "|     121|    Asset Allocation|Mutual Fund| 850.0|\n",
      "|     122|        Fixed Income|Mutual Fund| 268.0|\n",
      "+--------+--------------------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read in Productfile from HDFS\n",
    "Product = spark.read.csv('hdfs://localhost:54310/user/andrew/data/relational_example/Product.csv', header=True)\n",
    "\n",
    "\n",
    "#Strip dollar signs to make it numeric\n",
    "strip_dollar=udf(lambda s: s.replace(\"$\", \"\"))\n",
    "strip_comma=udf(lambda s: s.replace(\",\", \"\"))\n",
    "\n",
    "Product_2=Product.withColumn(\"Amount\", strip_dollar(\"Amount\"))\n",
    "Product_3=Product_2.withColumn(\"Amount\", strip_comma(\"Amount\"))\n",
    "Product_4=Product_3.withColumn(\"Amount\", Product_3[\"Amount\"].cast(\"double\"))\n",
    "\n",
    "#Register for SQL Use\n",
    "Product_4.createOrReplaceTempView(\"Product\")\n",
    "\n",
    "#See head\n",
    "Product_4.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+----------+------------------+\n",
      "|summary|         itemcode|              item|  category|            Amount|\n",
      "+-------+-----------------+------------------+----------+------------------+\n",
      "|  count|               34|                34|        34|                34|\n",
      "|   mean|            196.0|              null|      null| 314.9647058823529|\n",
      "| stddev|81.54604573955487|              null|      null|321.16340585756666|\n",
      "|    min|              111|       529 College|   Banking|               2.8|\n",
      "|    max|              324|Variable Universal|Retirement|            1595.0|\n",
      "+-------+-----------------+------------------+----------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('itemcode', 'string'), ('item', 'string'), ('category', 'string'), ('Amount', 'double')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the data typs\n",
    "Product_4.describe().show()\n",
    "Product_4.dtypes\n",
    "\n",
    "#Alternative syntax to replace\n",
    "#Product4=Product.withColumn(\"category\", regexp_replace(Product[\"category\"],\"Retir\",\"abc\"))\n",
    "#Product4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------+----------+-----------+---------+\n",
      "|order_number|quantity|itemcode|order_type|Customer_ID|     date|\n",
      "+------------+--------+--------+----------+-----------+---------+\n",
      "|        1002|       1|     324|         1|          2|02Jan2016|\n",
      "|        1002|       1|     322|         1|          2|02Jan2016|\n",
      "|        1001|       1|     322|         0|          1|02Jan2016|\n",
      "|        1003|       1|     324|         1|          3|04Jan2016|\n",
      "|        1004|       1|     324|         1|          4|04Jan2016|\n",
      "|        1005|       2|     314|         1|          5|04Jan2016|\n",
      "|        1006|       1|     314|         0|          6|05Jan2016|\n",
      "|        1007|       1|     323|         0|          7|05Jan2016|\n",
      "|        1008|       1|     323|         0|          8|08Jan2016|\n",
      "|        1009|       4|     221|         1|          9|08Jan2016|\n",
      "+------------+--------+--------+----------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read in Transaction file from HDFS\n",
    "Transaction = spark.read.csv('hdfs://localhost:54310/user/andrew/data/relational_example/Transaction.csv', header=True)\n",
    "\n",
    "#Register for SQL Use\n",
    "Transaction.createOrReplaceTempView(\"Transaction\")\n",
    "\n",
    "Transaction.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "|        |   customer|       true|\n",
      "|        |    product|       true|\n",
      "|        |transaction|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Join the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+---------+--------------------+---------------+--------+------+\n",
      "|CustomerID|Gender|State|     date|                item|       category|quantity|Amount|\n",
      "+----------+------+-----+---------+--------------------+---------------+--------+------+\n",
      "|         1|     M|   KY|17Jan2016|        Fixed Income|    Mutual Fund|       1| 268.0|\n",
      "|         1|     M|   KY|02Jan2016|           Refinance|         Borrow|       1|1595.0|\n",
      "|         2|     F|   ME|02Jan2016|           Refinance|         Borrow|       1|1595.0|\n",
      "|         2|     F|   ME|02Jan2016|       Grow my money|Managed Account|       1|  95.0|\n",
      "|         3|     M|   IN|11Feb2016|       Grow my money|Managed Account|       3|  95.0|\n",
      "|         3|     M|   IN|01Feb2016|       Grow my money|Managed Account|       3|  95.0|\n",
      "|         3|     M|   IN|28Jan2016|             SEP IRA|     Retirement|       1| 550.0|\n",
      "|         3|     M|   IN|04Jan2016|       Grow my money|Managed Account|       1|  95.0|\n",
      "|         4|     M|   NY|12Feb2016|       Grow my money|Managed Account|       1|  95.0|\n",
      "|         4|     M|   NY|02Feb2016|       Grow my money|Managed Account|       1|  95.0|\n",
      "|         4|     M|   NY|25Jan2016|      Personal Trust|Managed Account|       1|  72.0|\n",
      "|         4|     M|   NY|23Jan2016|             SEP IRA|     Retirement|       2| 550.0|\n",
      "|         4|     M|   NY|23Jan2016|              Keough|     Retirement|       2| 325.0|\n",
      "|         4|     M|   NY|04Jan2016|       Grow my money|Managed Account|       1|  95.0|\n",
      "|         5|     F|   NY|13Feb2016|Certificates of D...|        Banking|       2| 168.0|\n",
      "|         5|     F|   NY|03Feb2016|Certificates of D...|        Banking|       2| 168.0|\n",
      "|         5|     F|   NY|04Jan2016|Certificates of D...|        Banking|       2| 168.0|\n",
      "|         6|     M|   OH|14Feb2016|Certificates of D...|        Banking|       1| 168.0|\n",
      "|         6|     M|   OH|04Feb2016|Certificates of D...|        Banking|       1| 168.0|\n",
      "|         6|     M|   OH|11Jan2016|  High Yield Savings|        Banking|       1| 168.0|\n",
      "+----------+------+-----+---------+--------------------+---------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Merge all 3 tables\n",
    "complete_history = spark.sql('''\n",
    "                             SELECT t1.*,\n",
    "                                    t2.*,\n",
    "                                    t3.item,\n",
    "                                    t3.category,\n",
    "                                    t3.Amount\n",
    "                             FROM customer t1\n",
    "                             INNER JOIN transaction t2\n",
    "                             INNER JOIN product t3\n",
    "                             ON t1.CustomerID=t2.Customer_ID\n",
    "                             AND t2.itemcode=t3.itemcode''')\n",
    "\n",
    "complete_history[['CustomerID','Gender','State','date','item','category','quantity','Amount']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomerID', 'State', 'Gender', 'FirstName', 'LastName', 'Birth_Date', 'Address', 'Member_Type_ID', 'order_number', 'quantity', 'itemcode', 'order_type', 'Customer_ID', 'date', 'item', 'category', 'Amount']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_history.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+----------------+\n",
      "|CustomerID|order_count|basket_size|total_spend|avg_basket_spend|\n",
      "+----------+-----------+-----------+-----------+----------------+\n",
      "|         7|          2|        1.5|          3|           247.5|\n",
      "|        15|          2|        1.0|          2|            91.0|\n",
      "|        11|          3|        2.0|          6|           285.0|\n",
      "|        29|          1|        2.0|          2|           912.0|\n",
      "|        42|          1|        1.0|          1|           325.0|\n",
      "|         3|          4|        1.0|          4|          208.75|\n",
      "|        30|          1|        1.0|          1|          1595.0|\n",
      "|        34|          1|        2.0|          2|          1448.0|\n",
      "|         8|          4|       2.25|          9|           299.0|\n",
      "|        28|          1|        2.0|          2|           144.0|\n",
      "|        22|          1|        1.0|          1|            17.0|\n",
      "|        16|          1|        1.0|          1|           165.0|\n",
      "|        35|          1|        1.0|          1|           168.0|\n",
      "|        47|          1|        2.0|          2|           500.0|\n",
      "|        43|          1|        2.0|          2|           698.0|\n",
      "|         5|          2|        1.5|          3|           252.0|\n",
      "|        31|          1|        1.0|          1|            49.0|\n",
      "|        18|          1|        1.0|          1|           268.0|\n",
      "|        27|          1|        2.0|          2|           622.0|\n",
      "|        17|          2|        1.5|          3|          1014.0|\n",
      "+----------+-----------+-----------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Customer Transaction History\n",
    "customer_history = spark.sql('''\n",
    "                             SELECT t1.CustomerID,\n",
    "                                    COUNT(distinct t2.order_number) AS order_count,\n",
    "                                    COUNT(*)/COUNT(distinct t2.order_number) AS basket_size,\n",
    "                                    count(t3.Amount) AS total_spend,\n",
    "                                    SUM(t3.Amount)/COUNT(distinct t2.order_number) AS avg_basket_spend\n",
    "                             FROM customer t1\n",
    "                             INNER JOIN transaction t2\n",
    "                             INNER JOIN product t3\n",
    "                             ON t1.CustomerID=t2.Customer_ID\n",
    "                             AND t2.itemcode=t3.itemcode\n",
    "                             GROUP BY t1.CustomerID''')\n",
    "\n",
    "customer_history.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
